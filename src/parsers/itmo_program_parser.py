# src/parsers/complete_itmo_parser.py

import asyncio
import aiohttp
import json
import re
from bs4 import BeautifulSoup
from datetime import datetime
from pathlib import Path
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

# PDF –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    import PyPDF2
    import pdfplumber
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("‚ö†Ô∏è PDF –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install PyPDF2 pdfplumber")

# === –ü–ê–†–°–ò–ù–ì –í–ï–ë-–°–¢–†–ê–ù–ò–¶ ===

def parse_directions_data(text):
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏"""
    directions = []
    pattern = r'(\d{2}\.\d{2}\.\d{2})([–ê-–Ø–∞-—è—ë\s]+?)(\d+)–±—é–¥–∂–µ—Ç–Ω—ã—Ö(\d+)—Ü–µ–ª–µ–≤–∞—è(\d+)–∫–æ–Ω—Ç—Ä–∞–∫—Ç–Ω—ã—Ö'
    
    matches = re.finditer(pattern, text)
    for match in matches:
        direction = {
            'code': match.group(1).strip(),
            'name': match.group(2).strip(),
            'budget_places': int(match.group(3)),
            'target_places': int(match.group(4)),
            'contract_places': int(match.group(5))
        }
        directions.append(direction)
    
    return directions

def parse_table_col_data(text):
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Information_table__col__8wJDy"""
    patterns = [
        (r'—Ñ–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è(.+)', '—Ñ–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è'),
        (r'–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å(.+)', '–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å'),
        (r'—è–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è(.+)', '—è–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è'),
        (r'—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è \(–≥–æ–¥\)(.+)', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–≥–æ–¥)'),
        (r'–æ–±—â–µ–∂–∏—Ç–∏–µ(.+)', '–æ–±—â–µ–∂–∏—Ç–∏–µ'),
        (r'–≤–æ–µ–Ω–Ω—ã–π —É—á–µ–±–Ω—ã–π —Ü–µ–Ω—Ç—Ä(.+)', '–≤–æ–µ–Ω–Ω—ã–π —É—á–µ–±–Ω—ã–π —Ü–µ–Ω—Ç—Ä'),
        (r'–≥–æ—Å\. –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏—è(.+)', '–≥–æ—Å. –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏—è'),
        (r'–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏(.+)', '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏')
    ]
    
    result = {}
    for pattern, key in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            value = match.group(1).strip()
            result[key] = value
    
    return result

def extract_web_data(soup):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    result = {
        'program_title': '',
        'directions': [],
        'basic_info': {},
        'manager_name': '',
        'manager_contacts': [],
        'social_links': [],
        'pdf_links': []
    }
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    header_element = soup.find(class_='Information_information__header__fab3I')
    if header_element:
        result['program_title'] = header_element.get_text().strip()
    
    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    directions_text = ""
    for direction_element in soup.find_all(class_='Directions_directions__edkEZ'):
        directions_text += direction_element.get_text().strip()
    
    if directions_text:
        result['directions'] = parse_directions_data(directions_text)
    
    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    for element in soup.find_all(class_='Information_table__col__8wJDy'):
        text = element.get_text().strip()
        parsed_data = parse_table_col_data(text)
        result['basic_info'].update(parsed_data)
    
    # –ò–º—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    manager_name_element = soup.find(class_='Information_manager__name__ecPmn')
    if manager_name_element:
        result['manager_name'] = manager_name_element.get_text().strip()
    
    # –ö–æ–Ω—Ç–∞–∫—Ç—ã –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    for contact_element in soup.find_all(class_='Information_manager__contact__1fPAH'):
        contact_text = contact_element.get_text().strip()
        if contact_text:
            result['manager_contacts'].append(contact_text)
    
    # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    for social_element in soup.find_all(class_='Information_socials__link___eN3E'):
        social_data = {
            'text': social_element.get_text().strip(),
            'href': social_element.get('href', '') if social_element.name == 'a' else ''
        }
        
        if not social_data['href'] and social_element.find('a'):
            link = social_element.find('a')
            social_data['href'] = link.get('href', '')
        
        result['social_links'].append(social_data)
    
    # PDF —Å—Å—ã–ª–∫–∏
    for pdf_link in soup.find_all('a', href=lambda x: x and x.endswith('.pdf')):
        result['pdf_links'].append({
            'text': pdf_link.get_text().strip(),
            'href': pdf_link.get('href', '')
        })
    
    return result

# === –ü–ê–†–°–ò–ù–ì PDF ===

def extract_text_from_pdf(pdf_path):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
    if not PDF_AVAILABLE:
        return ""
    
    text = ""
    
    # –ü—Ä–æ–±—É–µ–º pdfplumber
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        if text.strip():
            return text
    except Exception as e:
        print(f"pdfplumber –æ—à–∏–±–∫–∞: {e}")
    
    # –ü—Ä–æ–±—É–µ–º PyPDF2
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        print(f"PyPDF2 –æ—à–∏–±–∫–∞: {e}")
    
    return text

def parse_curriculum_text(text):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞"""
    # –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    program_name = ""
    lines = text.split('\n')[:10]
    for line in lines:
        line = line.strip()
        if ('–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç' in line.lower() or 
            '–∏–∏' in line.lower() and '–ø—Ä–æ–≥—Ä–∞–º–º–∞' in line.lower()):
            program_name = line
            break
    
    if not program_name:
        program_name = "–£—á–µ–±–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞"
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤ —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞
    blocks = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –±–ª–æ–∫–æ–≤
    block_patterns = [
        r'–ë–ª–æ–∫\s+(\d+)\.?\s*([^\n]+)\s+(\d+)\s+(\d+)',
        r'(–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã)[.\s]*(\d+)\s+(\d+)',
        r'(–ü—É–ª –≤—ã–±–æ—Ä–Ω—ã—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω)[.\s]*(\d+)\s+(\d+)',
        r'(–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è.*–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞)\s+(\d+)\s+(\d+)',
        r'(–ü—Ä–∞–∫—Ç–∏–∫–∞)\s+(\d+)\s+(\d+)',
        r'(–ì–ò–ê)\s+(\d+)\s+(\d+)'
    ]
    
    for pattern in block_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE)
        
        for match in matches:
            groups = match.groups()
            
            if len(groups) >= 3:
                if len(groups) == 4:
                    block_name = f"–ë–ª–æ–∫ {groups[0]}. {groups[1]}"
                    credits = int(groups[2])
                    hours = int(groups[3])
                else:
                    block_name = groups[0]
                    credits = int(groups[1])
                    hours = int(groups[2])
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—É—Ä—Å—ã –¥–ª—è –±–ª–æ–∫–∞
                courses = extract_courses_for_block(text, match.end(), block_name)
                
                block = {
                    'name': block_name,
                    'total_credits': credits,
                    'total_hours': hours,
                    'courses': courses
                }
                blocks.append(block)
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –±–ª–æ–∫–∏, –ø–∞—Ä—Å–∏–º –≤—Å–µ –∫—É—Ä—Å—ã
    if not blocks:
        all_courses = extract_courses_from_text(text)
        if all_courses:
            total_credits = sum(course.get('credits', 0) for course in all_courses)
            blocks.append({
                'name': '–£—á–µ–±–Ω—ã–π –ø–ª–∞–Ω',
                'total_credits': total_credits,
                'total_hours': total_credits * 36,
                'courses': all_courses
            })
    
    return {
        'program_name': program_name,
        'blocks': blocks,
        'total_credits': sum(block['total_credits'] for block in blocks),
        'total_courses': sum(len(block['courses']) for block in blocks)
    }

def extract_courses_for_block(text, start_pos, block_name):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ–≤ –¥–ª—è –±–ª–æ–∫–∞"""
    block_text = text[start_pos:]
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±–ª–∞—Å—Ç—å –ø–æ–∏—Å–∫–∞ —Å–ª–µ–¥—É—é—â–∏–º –±–ª–æ–∫–æ–º
    next_block_patterns = [
        r'–ë–ª–æ–∫\s+\d+',
        r'–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è.*–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞',
        r'–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã',
        r'–ü—É–ª –≤—ã–±–æ—Ä–Ω—ã—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω'
    ]
    
    end_pos = len(block_text)
    for pattern in next_block_patterns:
        match = re.search(pattern, block_text, re.IGNORECASE)
        if match and match.start() > 100:
            end_pos = match.start()
            break
    
    relevant_text = block_text[:end_pos]
    return extract_courses_from_text(relevant_text)

def extract_courses_from_text(text):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    courses = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫—É—Ä—Å–æ–≤
    course_patterns = [
        r'^(\d+)\s+([–ê-–Ø–∞-—è—ë–Å\w\s\-\(\)/\.,]+?)\s+(\d+)\s+(\d+)$',
        r'^([–ê-–Ø–∞-—è—ë–Å\w\s\-\(\)/\.,]{15,}?)\s+(\d+)\s+(\d+)$',
        r'^\d+\s+([–ê-–Ø–∞-—è—ë–Å\w\s\-\(\)/\.,]+?)\s+(\d+)\s+\d+$'
    ]
    
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        
        if len(line) < 10 or is_header_line(line):
            continue
        
        for pattern in course_patterns:
            match = re.match(pattern, line, re.IGNORECASE)
            if match:
                course = parse_course_match(match, line)
                if course:
                    courses.append(course)
                    break
    
    return courses

def parse_course_match(match, original_line):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫—É—Ä—Å–∞"""
    groups = match.groups()
    
    try:
        if len(groups) == 4:
            semester = int(groups[0])
            name = groups[1].strip()
            credits = int(groups[2])
            hours = int(groups[3])
        elif len(groups) == 3:
            name = groups[0].strip()
            credits = int(groups[1])
            hours = int(groups[2])
            semester = None
        else:
            return None
        
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        name = clean_course_name(name)
        
        if len(name) < 5 or credits <= 0:
            return None
        
        return {
            'name': name,
            'credits': credits,
            'hours': hours,
            'semester': semester
        }
        
    except (ValueError, IndexError):
        return None

def clean_course_name(name):
    """–û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–∞"""
    name = re.sub(r'\s+', ' ', name.strip())
    name = re.sub(r'^\d+\.?\s*', '', name)
    name = name.rstrip('/.').strip()
    return name

def is_header_line(line):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–æ—á–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    header_patterns = [
        r'^–ë–ª–æ–∫\s+\d+',
        r'^–°–µ–º–µ—Å—Ç—Ä',
        r'^–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ',
        r'^–¢—Ä—É–¥–æ–µ–º–∫–æ—Å—Ç—å',
        r'^\d+\s+\d+$',
        r'^–£—á–µ–±–Ω—ã–π –ø–ª–∞–Ω',
        r'^–û–ü\s+',
        r'^–ü—É–ª –≤—ã–±–æ—Ä–Ω—ã—Ö',
        r'^–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ'
    ]
    
    for pattern in header_patterns:
        if re.match(pattern, line, re.IGNORECASE):
            return True
    
    return False

# === –û–°–ù–û–í–ù–û–ô –ü–ê–†–°–ï–† ===

class CompleteITMOParser:
    """–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ò–¢–ú–û —Å –≤–µ–± –∏ PDF"""
    
    def __init__(self):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
        current_dir = Path(__file__).resolve()
        project_root = current_dir.parent.parent.parent  # src/parsers -> src -> project_root
        
        self.pdf_dir = project_root / "data" / "pdf"
        self.output_dir = project_root / "data" / "parsed"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.pdf_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.programs = {
            'ai': 'https://abit.itmo.ru/program/master/ai',
            'ai_product': 'https://abit.itmo.ru/program/master/ai_product'
        }
    
    async def parse_program(self, program_id, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
        print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º—ã: {program_id}")
        
        result = {
            'program_id': program_id,
            'url': url,
            'parsed_at': datetime.now().isoformat(),
            'web_data': None,
            'curriculum_data': None
        }
        
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
        web_data = await self.parse_web_page(url)
        if web_data:
            result['web_data'] = web_data
            print(f"‚úÖ –í–µ–±-–¥–∞–Ω–Ω—ã–µ: {web_data['program_title']}")
        
        # 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ PDF
        if web_data and web_data['pdf_links']:
            for pdf_link in web_data['pdf_links']:
                if '—É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω' in pdf_link['text'].lower():
                    pdf_url = pdf_link['href']
                    if not pdf_url.startswith('http'):
                        pdf_url = 'https://abit.itmo.ru' + pdf_url
                    
                    curriculum_data = await self.parse_curriculum_pdf(pdf_url, program_id)
                    if curriculum_data:
                        result['curriculum_data'] = curriculum_data
                        print(f"‚úÖ PDF –¥–∞–Ω–Ω—ã–µ: {curriculum_data['total_courses']} –∫—É—Ä—Å–æ–≤")
                    break
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF —Ñ–∞–π–ª–æ–≤
        if not result['curriculum_data']:
            local_pdf = self.find_local_pdf(program_id)
            if local_pdf:
                curriculum_data = self.parse_local_pdf(local_pdf)
                if curriculum_data:
                    result['curriculum_data'] = curriculum_data
                    print(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–π PDF: {curriculum_data['total_courses']} –∫—É—Ä—Å–æ–≤")
        
        return result
    
    async def parse_web_page(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        try:
            async with aiohttp.ClientSession(headers=headers) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        return extract_web_data(soup)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±-–ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        
        return None
    
    async def parse_curriculum_pdf(self, pdf_url, program_id):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ PDF"""
        if not PDF_AVAILABLE:
            print("‚ùå PDF –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            return None
        
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º PDF
            pdf_path = await self.download_pdf(pdf_url, program_id)
            if pdf_path:
                return self.parse_local_pdf(pdf_path)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ PDF –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        
        return None
    
    async def download_pdf(self, pdf_url, program_id):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —Ñ–∞–π–ª–∞"""
        filename = f"{program_id}_curriculum.pdf"
        pdf_path = self.pdf_dir / filename
        
        if pdf_path.exists():
            print(f"üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π PDF: {pdf_path}")
            return pdf_path
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with aiohttp.ClientSession(headers=headers) as session:
                async with session.get(pdf_url) as response:
                    if response.status == 200:
                        with open(pdf_path, 'wb') as f:
                            async for chunk in response.content.iter_chunked(8192):
                                f.write(chunk)
                        
                        print(f"üì• PDF —Å–∫–∞—á–∞–Ω: {pdf_path}")
                        return pdf_path
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF: {e}")
        
        return None
    
    def find_local_pdf(self, program_id):
        """–ü–æ–∏—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ PDF —Ñ–∞–π–ª–∞"""
        patterns = [
            f"{program_id}_curriculum.pdf",
            f"{program_id}.pdf",
            f"curriculum_{program_id}.pdf"
        ]
        
        for pattern in patterns:
            pdf_path = self.pdf_dir / pattern
            if pdf_path.exists():
                return pdf_path
        
        # –ü–æ–∏—Å–∫ –ª—é–±—ã—Ö PDF –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        for pdf_file in self.pdf_dir.glob("*.pdf"):
            if program_id in pdf_file.name.lower():
                return pdf_file
        
        return None
    
    def parse_local_pdf(self, pdf_path):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ PDF —Ñ–∞–π–ª–∞"""
        if not PDF_AVAILABLE:
            return None
        
        try:
            text = extract_text_from_pdf(pdf_path)
            if text:
                return parse_curriculum_text(text)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF {pdf_path}: {e}")
        
        return None
    
    async def parse_all_programs(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –ø—Ä–æ–≥—Ä–∞–º–º"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ò–¢–ú–û")
        
        all_results = {}
        
        for program_id, url in self.programs.items():
            result = await self.parse_program(program_id, url)
            all_results[program_id] = result
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await self.save_results(all_results)
        return all_results
    
    async def save_results(self, results):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        full_filename = self.output_dir / f"itmo_complete_{timestamp}.json"
        with open(full_filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
        summary = self.create_summary(results)
        summary_filename = self.output_dir / f"itmo_summary_{timestamp}.json"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –∫–∞–∫ "latest" –≤–µ—Ä—Å–∏–∏ (–≤–º–µ—Å—Ç–æ symlink –¥–ª—è Windows)
        latest_full = self.output_dir / "latest_complete.json"
        latest_summary = self.output_dir / "latest_summary.json"
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        import shutil
        shutil.copy2(full_filename, latest_full)
        shutil.copy2(summary_filename, latest_summary)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {full_filename}")
        print(f"   –°–≤–æ–¥–∫–∞: {summary_filename}")
        print(f"   –ü–æ—Å–ª–µ–¥–Ω–∏–µ –≤–µ—Ä—Å–∏–∏: latest_complete.json, latest_summary.json")
    
    def create_summary(self, results):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        summary = {
            'parsed_at': datetime.now().isoformat(),
            'programs_count': len(results),
            'programs': {}
        }
        
        for program_id, data in results.items():
            program_summary = {
                'program_id': program_id,
                'url': data['url'],
                'has_web_data': data['web_data'] is not None,
                'has_curriculum': data['curriculum_data'] is not None
            }
            
            if data['web_data']:
                program_summary.update({
                    'title': data['web_data']['program_title'],
                    'directions_count': len(data['web_data']['directions']),
                    'pdf_links_count': len(data['web_data']['pdf_links'])
                })
            
            if data['curriculum_data']:
                program_summary.update({
                    'total_credits': data['curriculum_data']['total_credits'],
                    'total_courses': data['curriculum_data']['total_courses'],
                    'blocks_count': len(data['curriculum_data']['blocks'])
                })
            
            summary['programs'][program_id] = program_summary
        
        return summary

# === –ó–ê–ü–£–°–ö ===

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üéì –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ò–¢–ú–û - –≤–µ–± + PDF")
    print("=" * 50)
    
    parser = CompleteITMOParser()
    results = await parser.parse_all_programs()
    
    print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–æ–≥—Ä–∞–º–º: {len(results)}")
    
    for program_id, data in results.items():
        web_status = "‚úÖ" if data['web_data'] else "‚ùå"
        pdf_status = "‚úÖ" if data['curriculum_data'] else "‚ùå"
        
        title = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        if data['web_data']:
            title = data['web_data']['program_title']
        
        print(f"  üìö {program_id}: {title}")
        print(f"     –í–µ–±: {web_status} | PDF: {pdf_status}")

if __name__ == '__main__':
    asyncio.run(main())